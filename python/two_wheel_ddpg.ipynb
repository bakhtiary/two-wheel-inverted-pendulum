{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae92aa14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from stable_baselines3 import DDPG\n",
    "from stable_baselines3.common.noise import NormalActionNoise, OrnsteinUhlenbeckActionNoise\n",
    "from rl_training.two_wheel_robot import TwoWheelRobot\n",
    "\n",
    "env = TwoWheelRobot()\n",
    "\n",
    "# The noise objects for DDPG\n",
    "n_actions = env.action_space.shape[-1]\n",
    "action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma=0.1 * np.ones(n_actions))\n",
    "\n",
    "model = DDPG(\"MlpPolicy\", env, action_noise=action_noise, verbose=1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "059b6ac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 25       |\n",
      "|    ep_rew_mean     | -418     |\n",
      "| time/              |          |\n",
      "|    episodes        | 10       |\n",
      "|    fps             | 135      |\n",
      "|    time_elapsed    | 1        |\n",
      "|    total_timesteps | 250      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 25.1     |\n",
      "|    critic_loss     | 227      |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 142      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 32.5     |\n",
      "|    ep_rew_mean     | -770     |\n",
      "| time/              |          |\n",
      "|    episodes        | 20       |\n",
      "|    fps             | 100      |\n",
      "|    time_elapsed    | 6        |\n",
      "|    total_timesteps | 650      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 71.6     |\n",
      "|    critic_loss     | 87.1     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 496      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50.8     |\n",
      "|    ep_rew_mean     | -467     |\n",
      "| time/              |          |\n",
      "|    episodes        | 30       |\n",
      "|    fps             | 87       |\n",
      "|    time_elapsed    | 17       |\n",
      "|    total_timesteps | 1525     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 51.9     |\n",
      "|    critic_loss     | 106      |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 1361     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 59.6     |\n",
      "|    ep_rew_mean     | -402     |\n",
      "| time/              |          |\n",
      "|    episodes        | 40       |\n",
      "|    fps             | 84       |\n",
      "|    time_elapsed    | 28       |\n",
      "|    total_timesteps | 2385     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 42.4     |\n",
      "|    critic_loss     | 81.9     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 2203     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 65.7     |\n",
      "|    ep_rew_mean     | -269     |\n",
      "| time/              |          |\n",
      "|    episodes        | 50       |\n",
      "|    fps             | 83       |\n",
      "|    time_elapsed    | 39       |\n",
      "|    total_timesteps | 3284     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 16.4     |\n",
      "|    critic_loss     | 39       |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 3090     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 71.5     |\n",
      "|    ep_rew_mean     | -162     |\n",
      "| time/              |          |\n",
      "|    episodes        | 60       |\n",
      "|    fps             | 82       |\n",
      "|    time_elapsed    | 51       |\n",
      "|    total_timesteps | 4289     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -6.92    |\n",
      "|    critic_loss     | 38       |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 4085     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 82.7     |\n",
      "|    ep_rew_mean     | -57.3    |\n",
      "| time/              |          |\n",
      "|    episodes        | 70       |\n",
      "|    fps             | 79       |\n",
      "|    time_elapsed    | 72       |\n",
      "|    total_timesteps | 5786     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -35.6    |\n",
      "|    critic_loss     | 65.3     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 5531     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 95.1     |\n",
      "|    ep_rew_mean     | 19       |\n",
      "| time/              |          |\n",
      "|    episodes        | 80       |\n",
      "|    fps             | 76       |\n",
      "|    time_elapsed    | 99       |\n",
      "|    total_timesteps | 7611     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -77.4    |\n",
      "|    critic_loss     | 32.6     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 7370     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 102      |\n",
      "|    ep_rew_mean     | 52.4     |\n",
      "| time/              |          |\n",
      "|    episodes        | 90       |\n",
      "|    fps             | 75       |\n",
      "|    time_elapsed    | 120      |\n",
      "|    total_timesteps | 9155     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -102     |\n",
      "|    critic_loss     | 42.5     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 8928     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 110      |\n",
      "|    ep_rew_mean     | 94.1     |\n",
      "| time/              |          |\n",
      "|    episodes        | 100      |\n",
      "|    fps             | 75       |\n",
      "|    time_elapsed    | 146      |\n",
      "|    total_timesteps | 11038    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -123     |\n",
      "|    critic_loss     | 38.7     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 10761    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 135      |\n",
      "|    ep_rew_mean     | 228      |\n",
      "| time/              |          |\n",
      "|    episodes        | 110      |\n",
      "|    fps             | 74       |\n",
      "|    time_elapsed    | 183      |\n",
      "|    total_timesteps | 13757    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -146     |\n",
      "|    critic_loss     | 33.5     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 13431    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 161      |\n",
      "|    ep_rew_mean     | 468      |\n",
      "| time/              |          |\n",
      "|    episodes        | 120      |\n",
      "|    fps             | 74       |\n",
      "|    time_elapsed    | 225      |\n",
      "|    total_timesteps | 16772    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -185     |\n",
      "|    critic_loss     | 64.6     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 16334    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | 713      |\n",
      "| time/              |          |\n",
      "|    episodes        | 130      |\n",
      "|    fps             | 75       |\n",
      "|    time_elapsed    | 284      |\n",
      "|    total_timesteps | 21572    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -269     |\n",
      "|    critic_loss     | 41.6     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 20897    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 232      |\n",
      "|    ep_rew_mean     | 909      |\n",
      "| time/              |          |\n",
      "|    episodes        | 140      |\n",
      "|    fps             | 74       |\n",
      "|    time_elapsed    | 345      |\n",
      "|    total_timesteps | 25608    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -358     |\n",
      "|    critic_loss     | 64.4     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 25105    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 244      |\n",
      "|    ep_rew_mean     | 904      |\n",
      "| time/              |          |\n",
      "|    episodes        | 150      |\n",
      "|    fps             | 73       |\n",
      "|    time_elapsed    | 378      |\n",
      "|    total_timesteps | 27686    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -390     |\n",
      "|    critic_loss     | 73.8     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 27372    |\n",
      "---------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_579511/3130689266.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# model.save(\"ddpg_two_wheel_robot\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# env = model.get_env()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# del model # remove to demonstrate saving and loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# model = DDPG.load(\"ddpg_two_wheel_robot\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/python-rQ1OrFAJ/lib/python3.9/site-packages/stable_baselines3/ddpg/ddpg.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps)\u001b[0m\n\u001b[1;32m    128\u001b[0m     ) -> OffPolicyAlgorithm:\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m         return super(DDPG, self).learn(\n\u001b[0m\u001b[1;32m    131\u001b[0m             \u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/python-rQ1OrFAJ/lib/python3.9/site-packages/stable_baselines3/td3/td3.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps)\u001b[0m\n\u001b[1;32m    202\u001b[0m     ) -> OffPolicyAlgorithm:\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m         return super(TD3, self).learn(\n\u001b[0m\u001b[1;32m    205\u001b[0m             \u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/python-rQ1OrFAJ/lib/python3.9/site-packages/stable_baselines3/common/off_policy_algorithm.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps)\u001b[0m\n\u001b[1;32m    369\u001b[0m                 \u001b[0;31m# Special case when the user passes `gradient_steps=0`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgradient_steps\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 371\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgradient_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m         \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_training_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/python-rQ1OrFAJ/lib/python3.9/site-packages/stable_baselines3/td3/td3.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, gradient_steps, batch_size)\u001b[0m\n\u001b[1;32m    168\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m             \u001b[0mcritic_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0;31m# Delayed policy updates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/python-rQ1OrFAJ/lib/python3.9/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/python-rQ1OrFAJ/lib/python3.9/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/python-rQ1OrFAJ/lib/python3.9/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    131\u001b[0m                     \u001b[0mstate_steps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'step'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m             F.adam(params_with_grad,\n\u001b[0m\u001b[1;32m    134\u001b[0m                    \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m                    \u001b[0mexp_avgs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/python-rQ1OrFAJ/lib/python3.9/site-packages/torch/optim/_functional.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0;31m# Maintains the maximum of all 2nd moment running avg. till now\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.learn(total_timesteps=100000, log_interval=10)\n",
    "# model.save(\"ddpg_two_wheel_robot\")\n",
    "# env = model.get_env()\n",
    "# del model # remove to demonstrate saving and loading\n",
    "# model = DDPG.load(\"ddpg_two_wheel_robot\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3b33373",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ddpg.ddpg.DDPG at 0x7f8ba8acfb20>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.learn(total_timesteps=200000, log_interval=10)\n",
    "model.load(\"ddpg_two_wheel_robot\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d1b7c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DDPG.load(\"ddpg_two_wheel_robot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "194d33cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'done_reward': 10, 'movement_reward': -0.0, 'action_reward': -2.18989}\n",
      "Creating window glfw\n",
      "{'done_reward': 10, 'movement_reward': -0.0, 'action_reward': -1.5634928}\n",
      "{'done_reward': 10, 'movement_reward': -2.051463293972429, 'action_reward': -1.5463982}\n",
      "{'done_reward': 10, 'movement_reward': -2.1262219553096573, 'action_reward': -10.858177}\n",
      "{'done_reward': 10, 'movement_reward': -2.080055095097097, 'action_reward': -2.2558298}\n",
      "{'done_reward': 10, 'movement_reward': -1.3832195596745425, 'action_reward': -0.44129467}\n",
      "{'done_reward': 10, 'movement_reward': -1.0962231315133197, 'action_reward': -0.23742199}\n",
      "{'done_reward': 10, 'movement_reward': -0.9997609011288661, 'action_reward': -0.35452652}\n",
      "{'done_reward': 10, 'movement_reward': -0.9743394408237281, 'action_reward': -0.31013775}\n",
      "{'done_reward': 10, 'movement_reward': -0.9608745680607417, 'action_reward': -0.10508156}\n",
      "{'done_reward': 10, 'movement_reward': -0.9592415748395242, 'action_reward': -0.31746578}\n",
      "{'done_reward': 10, 'movement_reward': -0.9389557902774572, 'action_reward': -0.042909622}\n",
      "{'done_reward': 10, 'movement_reward': -0.9335026925431761, 'action_reward': -0.35894585}\n",
      "{'done_reward': 10, 'movement_reward': -0.9135163376165402, 'action_reward': -0.075858116}\n",
      "{'done_reward': 10, 'movement_reward': -0.9094139753385453, 'action_reward': -0.37729645}\n",
      "{'done_reward': 10, 'movement_reward': -0.9002049963632406, 'action_reward': -0.061881065}\n",
      "{'done_reward': 10, 'movement_reward': -0.9005076935093252, 'action_reward': -0.29516697}\n",
      "{'done_reward': 10, 'movement_reward': -0.901487516892716, 'action_reward': -0.044133186}\n",
      "{'done_reward': 10, 'movement_reward': -0.9087272600240212, 'action_reward': -0.1700077}\n",
      "{'done_reward': 10, 'movement_reward': -0.9170467508093114, 'action_reward': -0.06548691}\n",
      "{'done_reward': 10, 'movement_reward': -0.9274291957638194, 'action_reward': -0.11962032}\n",
      "{'done_reward': 10, 'movement_reward': -0.9219150390489746, 'action_reward': -0.33455563}\n",
      "{'done_reward': 10, 'movement_reward': -0.8902855419487389, 'action_reward': -0.4382658}\n",
      "{'done_reward': 10, 'movement_reward': -0.847837892682228, 'action_reward': -0.4273491}\n",
      "{'done_reward': 10, 'movement_reward': -0.8076591365658786, 'action_reward': -0.43824863}\n",
      "{'done_reward': 10, 'movement_reward': -0.7690616385292445, 'action_reward': -0.6275215}\n",
      "{'done_reward': 10, 'movement_reward': -0.7128632579938223, 'action_reward': -0.71321774}\n",
      "{'done_reward': 10, 'movement_reward': -0.6512225963702274, 'action_reward': -1.0590944}\n",
      "{'done_reward': 10, 'movement_reward': -0.5531498908683096, 'action_reward': -1.2636175}\n",
      "{'done_reward': 10, 'movement_reward': -0.42473832030921777, 'action_reward': -0.95014954}\n",
      "{'done_reward': 10, 'movement_reward': -0.33102810396191956, 'action_reward': -1.1318598}\n",
      "{'done_reward': 10, 'movement_reward': -0.22696223702940993, 'action_reward': -1.0630512}\n",
      "{'done_reward': 10, 'movement_reward': -0.1539434015181749, 'action_reward': -1.121336}\n",
      "{'done_reward': 10, 'movement_reward': -0.15500645329614407, 'action_reward': -1.0789881}\n",
      "{'done_reward': 10, 'movement_reward': -0.21813008894127514, 'action_reward': -0.9569092}\n",
      "{'done_reward': 10, 'movement_reward': -0.2949258271225612, 'action_reward': -0.9778633}\n",
      "{'done_reward': 10, 'movement_reward': -0.3808844354205201, 'action_reward': -1.0563431}\n",
      "{'done_reward': 10, 'movement_reward': -0.4872531343327496, 'action_reward': -1.2309151}\n",
      "{'done_reward': 10, 'movement_reward': -0.6304705927980848, 'action_reward': -1.3989992}\n",
      "{'done_reward': 10, 'movement_reward': -0.8272585393215095, 'action_reward': -1.7132521}\n",
      "{'done_reward': 10, 'movement_reward': -1.0957604558965937, 'action_reward': -2.6607466}\n",
      "{'done_reward': 10, 'movement_reward': -1.5189488420307455, 'action_reward': -3.2453403}\n",
      "{'done_reward': 10, 'movement_reward': -2.019619716443641, 'action_reward': -2.3119812}\n",
      "{'done_reward': 10, 'movement_reward': -2.2237298094801763, 'action_reward': -1.6040659}\n",
      "{'done_reward': 10, 'movement_reward': -2.2583560182087807, 'action_reward': -1.2160873}\n",
      "{'done_reward': 10, 'movement_reward': -2.3416325741642137, 'action_reward': -1.5947285}\n",
      "{'done_reward': 10, 'movement_reward': -2.484468101890856, 'action_reward': -1.8271694}\n",
      "{'done_reward': 10, 'movement_reward': -2.6251799769269684, 'action_reward': -1.4512663}\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amir/.local/share/virtualenvs/python-rQ1OrFAJ/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3452: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "obs = env.reset()\n",
    "for i in range(400):\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, dones, info = env.step(action)\n",
    "    print(info)\n",
    "    env.render()\n",
    "    time.sleep(0.05)\n",
    "    if dones: \n",
    "      break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "253d6136",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c18e7c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "032f56ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()\n",
    "for i in range(100):\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, dones, info = env.step(action)\n",
    "    env.render()\n",
    "    time.sleep(0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b6d5a43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "0733158b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-100.0, True, {})"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "force = 0.0\n",
    "\n",
    "obs, rewards, dones, info = env.step((force,force))\n",
    "env.render()\n",
    "rewards, dones, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a180770",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
