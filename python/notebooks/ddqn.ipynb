{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I tried to get this to work on feb 13 2022 and it has a problem with values being on cuda and cpu at the same time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the continuous action space lunar lander problem you can no longer use the DQN.\n",
    "The DQN outputs a Q value prediction for each action at each state.\n",
    "\n",
    "With a continuous action space you can no longer do this. \n",
    "\n",
    "Instead here is a policy gradient implementation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Deterministic Policy Gradient\n",
    "\n",
    "Predict action given the input state\n",
    "\n",
    "Start an episode\n",
    "\n",
    "Use policy network with the state as input to predict the next action\n",
    "\n",
    "Take the action to get to the new state.\n",
    "\n",
    "Create a memory buffer of these transitions\n",
    "\n",
    "Have a critic network, calculate the q value for the state and action.\n",
    "Have a target critic network calculate the q value for the new state (use also the action at the new state decided by the actor_target network.\n",
    "\n",
    "Train the critic network based on the difference between these two. Update every so often. Temporal difference learning. \n",
    "\n",
    "Create the objective function - in this case you're trying to maximise the output of the critic\n",
    "\n",
    "Update the actor\n",
    "You just differentiate the critic wrt the actor (and go in the negative direction - gradient ascent)\n",
    "\n",
    "\n",
    "Best blog on pytorch and the fundamentals of backwards() and the graph \n",
    "https://blog.paperspace.com/pytorch-101-understanding-graphs-and-automatic-differentiation/\n",
    "\n",
    "https://medium.com/@thechrisyoon/deriving-policy-gradients-and-implementing-reinforce-f887949bd63\n",
    "\n",
    "https://towardsdatascience.com/deep-deterministic-policy-gradients-explained-2d94655a9b7b\n",
    "\n",
    "https://towardsdatascience.com/solving-lunar-lander-openaigym-reinforcement-learning-785675066197\n",
    "\n",
    "the total policy approaches summary\n",
    "https://lilianweng.github.io/lil-log/2018/04/08/policy-gradient-algorithms.html\n",
    "\n",
    "Good blog on DDPG\n",
    "https://pemami4911.github.io/blog/2016/08/21/ddpg-rl.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "import gym\n",
    "\n",
    "from torch.nn.functional import mse_loss\n",
    "from torch import optim\n",
    "\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting the device (CPU or GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the environment\n",
    "env = gym.make('LunarLanderContinuous-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test run of the lander.\n",
    "env.reset()\n",
    "for i in range(0,100):\n",
    "    env.render()\n",
    "    state, reward, done, _ = env.step(env.action_space.sample())\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experience replay\n",
    "\n",
    "class ReplayBuffer(object):\n",
    "    '''\n",
    "    This is a collection of transitions. (state, action reward sets)\n",
    "    inputs\n",
    "    size - defines the size of the storage.\n",
    "    memory - is the list that contains the transitions.\n",
    "    pointer - this identifies the current transition. \n",
    "    '''\n",
    "    \n",
    "    def __init__(self,size):\n",
    "        self.size = size\n",
    "        self.memory = []\n",
    "        self.pointer = 0\n",
    "        \n",
    "    def add_to_memory(self, transition):\n",
    "        if len(self.memory) == self.size:\n",
    "            self.memory[int(self.pointer)] = transition \n",
    "            self.pointer = (self.pointer + 1)% self.size\n",
    "        else:\n",
    "            self.memory.append(transition) \n",
    "            \n",
    "  # sample from the storage a batch\n",
    "    def sample(self, batch_size):\n",
    "        ind = np.random.randint(0, len(self.memory), batch_size)\n",
    "        # states, next statesm actions and rewards are placed in separate lists\n",
    "        # batch dones defines if the game is finished or not. \n",
    "        batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = [],[],[],[],[]\n",
    "        for i in ind:\n",
    "            state, next_state, action, reward, done = self.memory[i]\n",
    "            batch_states.append(np.array(state, copy=False))\n",
    "            batch_next_states.append(np.array(next_state, copy=False))\n",
    "            batch_actions.append(np.array(action, copy=False))\n",
    "            batch_rewards.append(np.array(reward, copy=False))\n",
    "            batch_dones.append(np.array(done, copy=False))\n",
    "\n",
    "        return np.array(batch_states), np.array(batch_next_states), np.array(batch_actions), np.array(batch_rewards).reshape(-1, 1), np.array(batch_dones).reshape(-1, 1)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The policy network\n",
    "\n",
    "class policy_net(nn.Module):\n",
    "    '''\n",
    "    state_dim - dimension of the state\n",
    "    action_dim - dimension of the action (note this doesn't mean \n",
    "    number of discrete actions, it means number of continuous values that represent the action space)\n",
    "    max_action - if you need to clip the action to a certain range\n",
    "    '''\n",
    "    def __init__(self, state_dim, action_dim, max_action):\n",
    "        super(policy_net, self).__init__()\n",
    "        self.max_action = max_action\n",
    "        self.main= nn.Sequential(\n",
    "        nn.Linear(state_dim, 400),\n",
    "        nn.Linear(400, 300),\n",
    "        nn.Linear(300, action_dim),\n",
    "        nn.Tanh()\n",
    "        # as tanh is between 0 and 1 if you multiply by max action you'll be between -max and +max action\n",
    "        )\n",
    "    \n",
    "    def forward(self, input):\n",
    "        output = self.main(input)\n",
    "        output = output*self.max_action\n",
    "        return output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# critic network\n",
    "\n",
    "class critic_net(nn.Module):\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(critic_net, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(state_dim + action_dim, 400),\n",
    "            nn.Linear(400, 300),\n",
    "            # for a combined action and state just output one Q value.\n",
    "            nn.Linear(300, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, state, action):\n",
    "        input_combined = torch.cat([state, action], 1)\n",
    "        output = self.main(input_combined)\n",
    "        return output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy(policy, eval_episodes=10):\n",
    "    avg_reward = 0.\n",
    "    for _ in range(eval_episodes):\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            \n",
    "         \n",
    "            state = torch.Tensor(obs.reshape(1, -1)).to(device)\n",
    "            action = policy(state).cpu().data.numpy().flatten()\n",
    "            \n",
    "            obs, reward, done, _ = env.step(action)\n",
    "            avg_reward += reward\n",
    "    avg_reward /= eval_episodes\n",
    "    print (\"---------------------------------------\")\n",
    "    print (\"Average Reward over the Evaluation Step: %f\" % (avg_reward))\n",
    "    print (\"---------------------------------------\")\n",
    "    return avg_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state dim 8\n",
      "action dim 2\n",
      "max action 1.0\n"
     ]
    }
   ],
   "source": [
    "state_dim = env.observation_space.shape[0]\n",
    "print('state dim {}'.format(state_dim))\n",
    "action_dim = env.action_space.shape[0]\n",
    "print('action dim {}'.format(action_dim))\n",
    "max_action = float(env.action_space.high[0])\n",
    "print('max action {}'.format(max_action))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = \"LunarLanderContinuous-v2\" # Name of a environment (set it to any Continous environment you want)\n",
    "seed = 0 # Random seed number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "Settings: TD3_LunarLanderContinuous-v2_0\n",
      "---------------------------------------\n"
     ]
    }
   ],
   "source": [
    "file_name = \"%s_%s_%s\" % (\"TD3\", env_name, str(seed))\n",
    "print (\"---------------------------------------\")\n",
    "print (\"Settings: %s\" % (file_name))\n",
    "print (\"---------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if not os.path.exists(\"./results\"):\n",
    "#    os.makedirs(\"./results\")\n",
    "#if save_models and not os.path.exists(\"./pytorch_models\"):\n",
    "#    os.makedirs(\"./pytorch_models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making a save method to save a trained model\n",
    "def save(actor, critic, filename, directory):\n",
    "    torch.save(actor.state_dict(), '%s/%s_actor.pth' % (directory, filename))\n",
    "    torch.save(critic.state_dict(), '%s/%s_critic.pth' % (directory, filename))\n",
    "\n",
    "# Making a load method to load a pre-trained model\n",
    "def load(actor,critic, filename, directory):\n",
    "    actor.load_state_dict(torch.load('%s/%s_actor.pth' % (directory, filename)))\n",
    "    critic.load_state_dict(torch.load('%s/%s_critic.pth' % (directory, filename)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(start_timesteps=1000,evaluation_frequency=1000,max_timesteps=1000,replay_buffer_size =5000, learning_rate = 0.001):\n",
    "    evaluations = []\n",
    "    total_timesteps = 0\n",
    "    timesteps_since_eval = 0\n",
    "    episode_num = 0\n",
    "    done = True\n",
    "    t0 = time.time()\n",
    "    \n",
    "    start_timesteps = start_timesteps # Number of iterations/timesteps before which the model randomly chooses an action, and after which it starts to use the policy network\n",
    "    eval_freq = evaluation_frequency\n",
    "    max_timesteps = max_timesteps\n",
    "    save_models = True\n",
    "\n",
    "    expl_noise  = 0.1\n",
    "\n",
    "    discount = 0.99\n",
    "\n",
    "    tau = 0.005\n",
    "\n",
    "    replay_buffer_size = replay_buffer_size\n",
    "    batch_size = 128\n",
    "    max_steps = 25000\n",
    "\n",
    "    policy_noise = 0.2 # STD of Gaussian noise added to the actions for the exploration purposes\n",
    "    noise_clip = 0.5\n",
    "\n",
    "    policy_freq = 2\n",
    "    \n",
    "    # create a policy network\n",
    "    actor = policy_net(state_dim, action_dim, max_action)\n",
    "    actor_target = policy_net(state_dim, action_dim, max_action)\n",
    "\n",
    "    actor_target.load_state_dict(actor.state_dict())\n",
    "\n",
    "    critic = critic_net(state_dim, action_dim)\n",
    "    critic_target = critic_net(state_dim, action_dim)\n",
    "\n",
    "    critic_target.load_state_dict(critic.state_dict())\n",
    "    # Need to copy over the original parameters from critic and actor to the targets.\n",
    "\n",
    "    #critic optimiser\n",
    "    critic_optimiser = optim.Adam(critic.parameters(), lr = learning_rate)\n",
    "    actor_optimiser = optim.Adam(actor.parameters(), lr = learning_rate)\n",
    "\n",
    "    # create the replay buffer\n",
    "    replay_buffer = ReplayBuffer(replay_buffer_size)\n",
    "    \n",
    "    start = datetime.datetime.now()\n",
    "    # Main training \n",
    "    while total_timesteps < max_timesteps:\n",
    "\n",
    "        #if the episode is done. \n",
    "        if done:\n",
    "\n",
    "            if total_timesteps != 0:\n",
    "                #print(\"Total Timesteps: {} Episode Num: {} Reward: {}\".format(total_timesteps, episode_num, episode_reward))\n",
    "\n",
    "                # Implement a training run!\n",
    "                # Optimise networks: Once a episode?\n",
    "                # Only sample if buffer full\n",
    "                if len(replay_buffer) > 0.9 * replay_buffer_size:\n",
    "                    # sample from buffer\n",
    "                    batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = replay_buffer.sample(batch_size)\n",
    "\n",
    "                    state = torch.Tensor(batch_states).to(device)\n",
    "                    next_state = torch.Tensor(batch_next_states).to(device)\n",
    "                    action = torch.Tensor(batch_actions).to(device)\n",
    "                    reward = torch.Tensor(batch_rewards).to(device)\n",
    "                    dones = torch.Tensor(batch_dones).to(device)\n",
    "\n",
    "                    #Critic\n",
    "                    # Think about the critic model\n",
    "                    # here we input the state and the action to get the q value\n",
    "                    # state and action are already tensors. \n",
    "                    q_value = critic.forward(state, action)\n",
    "\n",
    "                    # how to train the critic (use temporal difference?)\n",
    "                    # what's the reward + q_value at next state\n",
    "                    next_action = actor_target.forward(next_state)\n",
    "\n",
    "                    noise = torch.Tensor(batch_actions).data.normal_(0, policy_noise).to(device)\n",
    "                    # clip the noise\n",
    "                    noise = noise.clamp(-noise_clip, noise_clip)\n",
    "                    #add the guassian noise and then clip to the -max action and plus max action. \n",
    "                    next_action = (next_action + noise).clamp(-max_action, max_action)\n",
    "\n",
    "\n",
    "                    q_value_next_state = critic_target(next_state, next_action)\n",
    "                    y = reward + (discount * q_value_next_state).detach()\n",
    "                    # critic_loss - difference between y and q_value. \n",
    "                    # what type of loss metric to use. \n",
    "                    critic_loss = mse_loss(q_value, y)\n",
    "\n",
    "                    critic_optimiser.zero_grad()\n",
    "                    critic_loss.backward()\n",
    "                    critic_optimiser.step()\n",
    "\n",
    "                    # only update the actor every two iterations \n",
    "                    if episode_num % policy_freq == 0:\n",
    "\n",
    "                        #Actor\n",
    "                        # differentiate the critic with respect to the actor and ascend gradient.\n",
    "                        actor_loss = -critic.forward(state, actor.forward(state)).mean()\n",
    "                        #print('actor loss{}'.format(actor_loss))\n",
    "                        actor_optimiser.zero_grad()\n",
    "                        actor_loss.backward()\n",
    "                        actor_optimiser.step()\n",
    "\n",
    "                        #Critic Target\n",
    "                        # every x cycles update\n",
    "                        # polyak averaging?\n",
    "                        for param, target_param in zip(critic.parameters(), critic_target.parameters()):\n",
    "                            target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "\n",
    "                        #Actor Target\n",
    "                        # every y cycles update\n",
    "                        # polyak averaging?\n",
    "                        for param, target_param in zip(actor.parameters(), actor_target.parameters()):\n",
    "                            target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "\n",
    "\n",
    "            # We evaluate the episode and we save the policy\n",
    "            if timesteps_since_eval >= eval_freq:\n",
    "                timesteps_since_eval %= eval_freq\n",
    "                evaluations.append(evaluate_policy(actor))\n",
    "                save(actor, critic, file_name, directory=\"./pytorch_models\")\n",
    "                np.save(\"./results/%s\" % (file_name), evaluations)\n",
    "\n",
    "            obs = env.reset()\n",
    "\n",
    "            done= False\n",
    "\n",
    "\n",
    "            episode_reward = 0\n",
    "            episode_timesteps = 0\n",
    "            episode_num += 1\n",
    "\n",
    "        #If the epsiode is not done!\n",
    "        if total_timesteps < start_timesteps:\n",
    "            action = env.action_space.sample()\n",
    "        else: # After x timesteps, we switch to the model\n",
    "\n",
    "            state = torch.Tensor(obs.reshape(1, -1)).to(device)\n",
    "            action = actor.forward(state).cpu().data.numpy().flatten()\n",
    "\n",
    "            # If the explore_noise parameter is not 0, we add noise to the action and we clip it\n",
    "            if expl_noise != 0:\n",
    "                action = (action + np.random.normal(0, expl_noise, size=env.action_space.shape[0])).clip(env.action_space.low, env.action_space.high)\n",
    "\n",
    "        # Find the next state.\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "        # We increase the total reward\n",
    "        episode_reward += reward\n",
    "        # store transitions into the replay buffer\n",
    "        replay_buffer.add_to_memory((obs, next_state, action, reward, done))\n",
    "\n",
    "        # Move onto the next state\n",
    "        state = next_state\n",
    "        episode_timesteps += 1\n",
    "        total_timesteps += 1\n",
    "        timesteps_since_eval += 1\n",
    "\n",
    "    # We add the last policy evaluation to our list of evaluations and we save our model\n",
    "    evaluations.append(evaluate_policy(actor))\n",
    "    if save_models: save(actor, critic ,\"%s\" % (file_name), directory=\"./pytorch_models\")\n",
    "    np.save(\"./results/%s\" % (file_name), evaluations)\n",
    "\n",
    "    end = datetime.datetime.now()\n",
    "\n",
    "    print(end-start)\n",
    "    # Think about how to display the training improvements of not so we can test different settings. \n",
    "    plt.plot(range(0,len(evaluations)), evaluations)\n",
    "    plt.xlabel('evaluation number, based on the evaluation frequency')\n",
    "    plt.ylabel('evaluation reward')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_addmm)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2747873/419238075.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtraining\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mevaluation_frequency\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreplay_buffer_size\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_2747873/1754564949.py\u001b[0m in \u001b[0;36mtraining\u001b[0;34m(start_timesteps, evaluation_frequency, max_timesteps, replay_buffer_size, learning_rate)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m             \u001b[0;31m# If the explore_noise parameter is not 0, we add noise to the action and we clip it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_2747873/3959771777.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/python-rQ1OrFAJ/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/python-rQ1OrFAJ/lib/python3.9/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/python-rQ1OrFAJ/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/python-rQ1OrFAJ/lib/python3.9/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/python-rQ1OrFAJ/lib/python3.9/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1846\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1847\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_addmm)"
     ]
    }
   ],
   "source": [
    "training(start_timesteps=1000,evaluation_frequency=1000,max_timesteps=10000,replay_buffer_size =5000, learning_rate = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: -288.609213\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: -315.923739\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: -248.346649\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: -283.699229\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: -338.953006\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: -830.737857\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: -643.474488\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: -821.846901\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: -1075.447625\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: -713.230967\n",
      "---------------------------------------\n",
      "0:00:39.624028\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "training(start_timesteps=1000,evaluation_frequency=1000,max_timesteps=10000,replay_buffer_size =5000, learning_rate = 0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
